# 🧠 LoRA(Low-Rank Adaptation of Large Language Models)
> 큰 인공지능 모델을 전체 수정하지 않음.
> 작은 '추가 모듈'만 학습해서 효율적으로 성능을 향상시키는 방법

- 기존 가중치는 고정
- 저랭크 행렬 (A, B)만 학습
- 빠르고 가볍고 안정적인 미세조정 기법
  
---

## 🤔 왜 쓰나요?
| 일반 파인튜닝 😓 | LoRA 😊 |
|------------------|---------|
| 모델 전체 수정 🔧 | 작은 조각만 수정 🧩 |
| GPU 메모리 많이 먹음 💥 | 메모리 절약 💾 |
| 느림 🐢 | 빠름 ⚡ |
| 파일 크기 큼 📦 | 가벼움 ✨ |

🔥 LoRA는 똑똑한 AI를 더 가볍고 빠르게 만듬

---

y = W * x ← W는 무거운 가중치

근데 LoRA는 `W`를 직접 안 바꿈.  
대신 `A`랑 `B`라는 **작은 조각**만 학습해서 이렇게 덧붙임:

W' = W + A * B ← LoRA가 만든 보정값

- 원래 W는 그대로!
- A와 B는 저랭크(낮은 차원) 행렬이라서 아주 가벼움!  
- 이게 바로 "덧붙이기 학습"

---

## 🧩 비유로 쉽게 말하면

- 🍔 전체 햄버거(모델)를 다시 만들지 말고,  
- 🧀 치즈 한 장(LoRA)만 살짝 얹어서 새 맛을 내도록 함

---

## 📦 어디에 쓰는건가?

LoRA는 요즘 거의 모든 모델에 쓰여요:

- ✍️ GPT, ChatGPT, BERT (언어 모델)
- 🗣️ Whisper (음성 인식)
- 🖼️ Stable Diffusion (이미지 생성)
- 🧠 YOLO, ViT (시각 모델 일부)

---

## 🎙️ Whisper + LoRA 예시

Whisper는 여러 언어를 인식할 수 있는 음성 모델이에요.  
그런데 "한국어만 더 잘 듣게" 하고 싶다면?

✅ LoRA를 써서 한국어에 맞게 "보조 조각"만 살짝 학습!  
✅ 원래 모델 그대로, LoRA 모듈만 붙이면 한국어 성능 UP!

---
