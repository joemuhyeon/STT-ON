# ⚙️ 훈련 설정 상세 설명 (TrainingArguments)
Hugging Face의 Trainer를 사용할 때 학습 환경을 제어하는 TrainingArguments는, 음성 인식 모델의 성능과 효율에 큰 영향을 미칩니다. 아래는 주요 인자들을 설명/예시/장점/설정 이유로 나누어 상세히 기술한 내용입니다.

---

## 1. output_dir
- 설명 : 모델 체크포인트와 로그 파일을 저장할  폴더 경로 입니다. 학습 도중 저장된 모델을 불러오거나 재개할 때 사용됩니다.
- 예시 : "./checkpoint" → 현재 작업 디렉토리 안에 checkpoint 폴더 생성
- 장점: 학습 중단 후 이어서 재학습 가능, 여러 실험 버전 관리 기능
- 설정 이유: 체크포인트와 로깅 데이터를 일괄적으로 저장하고 관리하기 위해 폴더를 따로 지정함.

---

## 2. per_device_train_batch_size
- 설명 : GPU 한 대에서 학습할 때 한 번에 처리할 샘플 수
- 예시 : 16 → GPU 하나가 1 step에 처리하는 오디오 샘플 수
- 장점 : 적절한 배치 크기는 학습 속도를 높이고 안정적인 학습을 가능하게 함
- 설정 이유 : 16은 일반적인 음성 모델(wav2vec2, Whisper 등)에서 GPU 메모리를 안정적으로 활용할 수 있는 크기이며, 병렬 학습도 고려한 값

---

## 3. per_device_eval_batch_size
- 설명 : 평가(validation) 시 GPU 한 대당 처리할 샘플 수
- 예시 : 8 → 검증 시 한 번에 처리할 샘플 수
- 장점 : 검증 과정에서 메모리 초과를 방지하며, 과도한 연산 지연 없이 평가 가능
- 설정 이유: 평가 시에는 역전파가 없지만 입력 길이 변동이 심해 메모리 관리를 위해 훈련 배치보다 작게 설정

---

## 4. gradient_accumulation_steps
- 설명: 그래디언트(gradient)를 누적하여 큰 배치처럼 학습할 수 있게 해주는 설정
- 예시: 2 → 16 × 2 = 32개의 샘플을 하나의 배치처럼 처리
- 장점: 메모리 한계를 넘지 않으면서도 효과적으로 큰 배치처럼 학습 가능
- 설정 이유: 실질적인 전체 배치를 32(16x2)처럼 늘려 성능을 향상시키되, 메모리 오류를 방지하기 위함 <br>
&gradient = 벡터 공간에서 스칼라 함수의 최대 증가율을 나타내는 벡터.

---

## 5. learning_rate (학습률)
- 설명: 모델이 학습할 때 가중치를 얼마나 크게 조정할지를 결정하는 값 -> 모델이 얼마나 빠르게/느리게 학습할지 조절하는 조절기
- 예시: 3e-4(0.0003)
- 장점: 적절한 학습률은 빠르게 수렴하면서도 안정적인 학습을 가능하게 함
- 설정 이유: 사전 학습된 음성 모델의 파인튜닝에 적합한 범위(1e-4~5e-4) 중 가장 안정적인 값으로 설정 -> 손실을 최소화

---

## 6. warmup_steps
- 설명: 학습 초기 일정 스텝 동안 학습률을 선형 증가시키는 단계
- 예시: 500  → 학습 시작 후 500 step 동안 학습률이 0 → 0.0003까지 선형 증가
- 장점: 초기 손실값이 튀는 것을 방지하고, 학습이 천천히 안정적으로 시작되도록 유도
- 설정 이유: 전체 학습 스텝 수 대비 3~10% 수준이 적절하며, 모델 구조가 크기 때문에 워밍업이 중요

---

## 7. num_train_epochs
- 설명: 전체 데이터셋을 몇 번 반복해서 학습할지 설정
- 예시 숫자: 10 → 학습 데이터를 10번 반복해서 전체 학습 진행
- 장점: 충분한 반복을 통해 모델이 수렴 가능
- 이 숫자의 의미:
-- 1 epoch 당 전체 학습 예제가 한 번씩 모두 사용됨
-- epoch 수가 높을수록 오버피팅 가능성 증가
- 설정 이유: 데이터셋의 크기와 모델 복잡도를 고려해 적절한 반복 횟수 설정








